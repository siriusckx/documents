# 1 功能
> tokenizer库是一个专门用于分词（token）的字符串处理库，可以使用简单易用的方法把一个字符串分解成若干个单词，它与string_algo库的分割算法很类似，但有更多的变化。  

> 引用的头文件 
```
#include <boost/tokenizer.hpp>
```